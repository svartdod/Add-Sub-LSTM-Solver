{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Training_Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9See8Q6qI_5"
      },
      "source": [
        "#Deep Learning LSTM Trainging + TEST PART \n",
        "#HARROUZ MOUAD Faculty of Science and Technology MASTER2 ISICG 20208044\n",
       
        "from random import seed\n",
        "from random import randint\n",
        "from numpy import array\n",
        "from math import ceil\n",
        "from math import log10\n",
        "from numpy import argmax\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import RepeatVector\n",
        "import tensorflow"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auZV6WgHwVjh"
      },
      "source": [
        "#Global Variables \n",
        "Equations = []"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbqugYMPqUQ3"
      },
      "source": [
        "# generate lists of random integers and their sum\n",
        "def random_sum_pairs(n_examples, n_numbers, largest):\n",
        "    X, y = list(), list()\n",
        "    for i in range(n_examples):\n",
        "        in_pattern = [randint(1, largest) for _ in range(n_numbers)]\n",
        "        out_pattern = sum(in_pattern)\n",
        "        X.append(in_pattern)\n",
        "        y.append(out_pattern)\n",
        "    return X, y"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHAf-KQyqU4J"
      },
      "source": [
        "# generate lists of random integers and their substruction\n",
        "def random_sub_pairs(n_examples, n_numbers, largest):\n",
        "    X, y = list(), list()\n",
        "    for i in range(n_examples):\n",
        "        in_pattern = [randint(1, largest) for _ in range(n_numbers)]\n",
        "        out_pattern = in_pattern[0]\n",
        "        for i in range(1,len(in_pattern)):\n",
        "          out_pattern=out_pattern-in_pattern[i]\n",
        "        X.append(in_pattern)\n",
        "        y.append(out_pattern)\n",
        "    return X, y"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6JJ9Is5rs8R"
      },
      "source": [
        "# convert data to strings\n",
        "def to_string(X, y,X1, n_numbers, largest):\n",
        "    max_length = n_numbers * ceil(log10(largest + 1)) + n_numbers - 1\n",
        "    Xstr = list()\n",
        "    for pattern in X:\n",
        "        strp = '+'.join([str(n) for n in pattern])\n",
        "        strp = ''.join([' ' for _ in range(max_length - len(strp))]) + strp\n",
        "        Xstr.append(strp)\n",
        "    for pattern in X1:\n",
        "        strp = '-'.join([str(n) for n in pattern])\n",
        "        strp = ''.join([' ' for _ in range(max_length - len(strp))]) + strp\n",
        "        Xstr.append(strp)\n",
        "\n",
        "\n",
        "    max_length = ceil(log10(n_numbers * (largest + 1)))\n",
        "    ystr = list()\n",
        "    for pattern in y:\n",
        "        strp = str(pattern)\n",
        "        strp = ''.join([' ' for _ in range(max_length - len(strp))]) + strp\n",
        "        ystr.append(strp)\n",
        "    \n",
        "    return Xstr, ystr"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuD_3GoKsvGL"
      },
      "source": [
        "# integer encode strings\n",
        "def integer_encode(X, y, alphabet):\n",
        "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "    Xenc = list()\n",
        "    for pattern in X:\n",
        "        integer_encoded = [char_to_int[char] for char in pattern]\n",
        "        Xenc.append(integer_encoded)\n",
        "    yenc = list()\n",
        "    for pattern in y:\n",
        "        integer_encoded = [char_to_int[char] for char in pattern]\n",
        "        yenc.append(integer_encoded)\n",
        "    return Xenc, yenc"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fdcs7xNs7fD"
      },
      "source": [
        "def one_hot_encode(X, y, max_int):\n",
        "    Xenc = list()\n",
        "    for seq in X:\n",
        "        pattern = list()\n",
        "        for index in seq:\n",
        "            vector = [0 for _ in range(max_int)]\n",
        "            vector[index] = 1\n",
        "            pattern.append(vector)\n",
        "        Xenc.append(pattern)\n",
        "    yenc = list()\n",
        "    for seq in y:\n",
        "        pattern = list()\n",
        "        for index in seq:\n",
        "            vector = [0 for _ in range(max_int)]\n",
        "            vector[index] = 1\n",
        "            pattern.append(vector)\n",
        "        yenc.append(pattern)\n",
        "    return Xenc, yenc"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfdDwMcYqaFU"
      },
      "source": [
        "# generate an encoded dataset\n",
        "def generate_data(n_samples,n_samples_substruction, n_numbers, largest, alphabet):\n",
        "    # generate pairs\n",
        "    global Equations \n",
        "    \n",
        "    X, y = random_sum_pairs(n_samples, n_numbers, largest)\n",
        "    X1,y1 = random_sub_pairs(n_samples_substruction, n_numbers, largest)\n",
        "   \n",
        "    # convert to strings\n",
        "    X, y = to_string(X, y+y1,X1, n_numbers, largest)\n",
        "    Equations=[X[i] for i in range(len(X))] \n",
        "    # integer encode\n",
        "    X, y = integer_encode(X, y, alphabet)\n",
        "   \n",
        "    # one hot encode\n",
        "    X, y = one_hot_encode(X, y, len(alphabet))\n",
        " \n",
        "    # return as numpy arrays\n",
        "    X, y = array(X), array(y)\n",
        "    return X, y\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWzzTVTVtHCy"
      },
      "source": [
        "# invert encoding\n",
        "def invert(seq, alphabet):\n",
        "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "    strings = list()\n",
        "    for pattern in seq:\n",
        "        string = int_to_char[argmax(pattern)]\n",
        "        strings.append(string)\n",
        "    return ''.join(strings)\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSW5yZdFqjbg",
        "outputId": "71002e57-d6af-4790-d8b5-1509474454f3"
      },
      "source": [
        "# define dataset\n",
        "seed(1)\n",
        "n_samples_addition = 10000\n",
        "n_samples_substruction = 5000\n",
        "n_numbers = 2\n",
        "largest = 100\n",
        "alphabet = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-', ' ','+']\n",
        "n_chars = len(alphabet)\n",
        "n_in_seq_length = n_numbers * ceil(log10(largest + 1)) + n_numbers - 1\n",
        "\n",
        "n_out_seq_length = ceil(log10(n_numbers * (largest + 1)))\n",
        "# define LSTM configuration\n",
        "n_batch = 10\n",
        "n_epoch = 50\n",
        "print(\"===== Alphabet Possible ====\")\n",
        "print(\"===== Max number in generation of numbers is: \"+str(largest)+\" ====\")\n",
        "print(\"===== Number composed of \"+str(n_numbers)+\" numbers ====\")\n",
        "print(\"===== we have \"+str(n_samples_addition) +\" couples for addition  and \"+str(n_samples_substruction)+\" for substraction training paradigms ====\")\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Alphabet Possible ====\n",
            "===== Max number in generation of numbers is: 100 ====\n",
            "===== Number composed of 2 numbers ====\n",
            "===== we have 10000 couples for addition  and 5000 for substraction training paradigms ====\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2Nw0yHxqpuK",
        "outputId": "f58500bb-2912-444b-a28a-97c33edc741a"
      },
      "source": [
        "# create LSTM\n",
        "print(\"===== Creating of LSTM ====\")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, input_shape=(n_in_seq_length, n_chars)))\n",
        "model.add(RepeatVector(n_out_seq_length))\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(n_chars, activation='softmax')))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'],run_eagerly=True)\n",
        "print(model.summary())\n",
        "print(\"=====  LSTM Created ====\")\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Creating of LSTM ====\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_6 (LSTM)               (None, 100)               45600     \n",
            "                                                                 \n",
            " repeat_vector_3 (RepeatVect  (None, 3, 100)           0         \n",
            " or)                                                             \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 3, 50)             30200     \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, 3, 13)            663       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 76,463\n",
            "Trainable params: 76,463\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "=====  LSTM Created ====\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ6-tyzRtLXk",
        "outputId": "761b3cfe-2dd4-4e96-ae09-006227a9f5cc"
      },
      "source": [
        "# train LSTM\n",
        "for i in range(n_epoch):\n",
        "    X, y = generate_data(n_samples_addition,n_samples_substruction, n_numbers, largest, alphabet)\n",
        "    print(\"Epoch num : \",i)\n",
        "    model.fit(X, y, epochs=1, batch_size=n_batch,shuffle=True)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch num :  0\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 1.7375 - accuracy: 0.3305\n",
            "Epoch num :  1\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 1.4054 - accuracy: 0.4601\n",
            "Epoch num :  2\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 1.2086 - accuracy: 0.5419\n",
            "Epoch num :  3\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 1.1138 - accuracy: 0.5831\n",
            "Epoch num :  4\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 1.0440 - accuracy: 0.6086\n",
            "Epoch num :  5\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 0.9984 - accuracy: 0.6255\n",
            "Epoch num :  6\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 0.9595 - accuracy: 0.6411\n",
            "Epoch num :  7\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 0.8903 - accuracy: 0.6692\n",
            "Epoch num :  8\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 0.7616 - accuracy: 0.7206\n",
            "Epoch num :  9\n",
            "1500/1500 [==============================] - 41s 28ms/step - loss: 0.5882 - accuracy: 0.7976\n",
            "Epoch num :  10\n",
            "1500/1500 [==============================] - 41s 28ms/step - loss: 0.4444 - accuracy: 0.8646\n",
            "Epoch num :  11\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 0.3418 - accuracy: 0.9050\n",
            "Epoch num :  12\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.2656 - accuracy: 0.9294\n",
            "Epoch num :  13\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.2111 - accuracy: 0.9468\n",
            "Epoch num :  14\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.1631 - accuracy: 0.9602\n",
            "Epoch num :  15\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 0.1319 - accuracy: 0.9673\n",
            "Epoch num :  16\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.1120 - accuracy: 0.9722\n",
            "Epoch num :  17\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 0.0874 - accuracy: 0.9792\n",
            "Epoch num :  18\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0785 - accuracy: 0.9798\n",
            "Epoch num :  19\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 0.0707 - accuracy: 0.9817\n",
            "Epoch num :  20\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0594 - accuracy: 0.9840\n",
            "Epoch num :  21\n",
            "1500/1500 [==============================] - 41s 28ms/step - loss: 0.0532 - accuracy: 0.9857\n",
            "Epoch num :  22\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0512 - accuracy: 0.9857\n",
            "Epoch num :  23\n",
            "1500/1500 [==============================] - 41s 28ms/step - loss: 0.0388 - accuracy: 0.9895\n",
            "Epoch num :  24\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0457 - accuracy: 0.9869\n",
            "Epoch num :  25\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0418 - accuracy: 0.9879\n",
            "Epoch num :  26\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0428 - accuracy: 0.9876\n",
            "Epoch num :  27\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0282 - accuracy: 0.9923\n",
            "Epoch num :  28\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0418 - accuracy: 0.9874\n",
            "Epoch num :  29\n",
            "1500/1500 [==============================] - 41s 28ms/step - loss: 0.0294 - accuracy: 0.9918\n",
            "Epoch num :  30\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0365 - accuracy: 0.9897\n",
            "Epoch num :  31\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0311 - accuracy: 0.9914\n",
            "Epoch num :  32\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0277 - accuracy: 0.9917\n",
            "Epoch num :  33\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0250 - accuracy: 0.9936\n",
            "Epoch num :  34\n",
            "1500/1500 [==============================] - 41s 28ms/step - loss: 0.0367 - accuracy: 0.9882\n",
            "Epoch num :  35\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0161 - accuracy: 0.9960\n",
            "Epoch num :  36\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0297 - accuracy: 0.9918\n",
            "Epoch num :  37\n",
            "1500/1500 [==============================] - 41s 28ms/step - loss: 0.0240 - accuracy: 0.9932\n",
            "Epoch num :  38\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0256 - accuracy: 0.9926\n",
            "Epoch num :  39\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0279 - accuracy: 0.9919\n",
            "Epoch num :  40\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0165 - accuracy: 0.9954\n",
            "Epoch num :  41\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0222 - accuracy: 0.9941\n",
            "Epoch num :  42\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 0.0212 - accuracy: 0.9942\n",
            "Epoch num :  43\n",
            "1500/1500 [==============================] - 41s 28ms/step - loss: 0.0220 - accuracy: 0.9936\n",
            "Epoch num :  44\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0196 - accuracy: 0.9942\n",
            "Epoch num :  45\n",
            "1500/1500 [==============================] - 41s 28ms/step - loss: 0.0200 - accuracy: 0.9952\n",
            "Epoch num :  46\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0230 - accuracy: 0.9934\n",
            "Epoch num :  47\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0171 - accuracy: 0.9952\n",
            "Epoch num :  48\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 0.0139 - accuracy: 0.9964\n",
            "Epoch num :  49\n",
            "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0153 - accuracy: 0.9959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LS7kY1bLRD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9578b0d1-ad2e-4dae-f5d5-ad9baab7c459"
      },
      "source": [
        "#Saving model \n",
        "import time\n",
        "print(\"Saving Model ...\")\n",
        "model_name = 'OutputData/SUB_ADD'+str(time.time())+'.h5'\n",
        "model.save(model_name)\n",
        "print (\"=== Model saved === \")"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Model ...\n",
            "=== Model saved === \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkUDjZ1htoZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e88cb58b-4c09-4a2a-880a-b7eb112dd5d3"
      },
      "source": [
        "print(\"===== Testing some Random values  ====\")\n",
        "\n",
        "size=5\n",
        "size_substraction=5\n",
        "X, y = generate_data(size,size_substraction, n_numbers, largest, alphabet)\n",
        "result = model.predict(X, batch_size=n_batch, verbose=0)\n",
        "# calculate error\n",
        "expected = [invert(x, alphabet) for x in y]\n",
        "predicted = [invert(x, alphabet) for x in result]\n",
        "# show some examples\n",
        "for i in range(size+size_substraction):\n",
        "    print('Equation : %s ,Expected=%s, Predicted=%s' % (Equations[i],expected[i], predicted[i]))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Testing some Random values  ====\n",
            "Equation :   96+60 ,Expected=156, Predicted=156\n",
            "Equation :    8+46 ,Expected= 54, Predicted= 54\n",
            "Equation :    4+77 ,Expected= 81, Predicted= 81\n",
            "Equation :   32+59 ,Expected= 91, Predicted= 91\n",
            "Equation :   56+34 ,Expected= 90, Predicted= 90\n",
            "Equation :   72-99 ,Expected=-27, Predicted=-27\n",
            "Equation :  23-100 ,Expected=-77, Predicted=-77\n",
            "Equation :   45-29 ,Expected= 16, Predicted= 16\n",
            "Equation :   29-27 ,Expected=  2, Predicted=  2\n",
            "Equation :    34-7 ,Expected= 27, Predicted= 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I73dY8Rl4Fi4"
      },
      "source": [
        "#To string for element \n",
        "def to_string_test(X,X1, n_numbers, largest):\n",
        "    max_length = n_numbers * ceil(log10(largest + 1)) + n_numbers - 1\n",
        "    Xstr = list()\n",
        "    for pattern in X:\n",
        "        strp = '+'.join([str(n) for n in pattern])\n",
        "        strp = ''.join([' ' for _ in range(max_length - len(strp))]) + strp\n",
        "        Xstr.append(strp)\n",
        "    for pattern in X1:\n",
        "        strp = '-'.join([str(n) for n in pattern])\n",
        "        strp = ''.join([' ' for _ in range(max_length - len(strp))]) + strp\n",
        "        Xstr.append(strp)\n",
        "    return Xstr"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW-df-xn3rAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd9203d-68cd-4321-b432-75b6d9c13765"
      },
      "source": [
        "#Do some Test\n",
        "X=[[10,20]]\n",
        "X1=[[25,16]]\n",
        "X=to_string_test(X,X1, n_numbers, largest)\n",
        "X, _ = integer_encode(X, [], alphabet)\n",
        "X, _ = one_hot_encode(X, [], len(alphabet))\n",
        "X = array(X)\n",
        "result = model.predict(X, batch_size=n_batch, verbose=0)\n",
        "predicted = [invert(x, alphabet) for x in result]\n",
        "for i in range(2):\n",
        "    print(' Predicted=%s' % (predicted[i]))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Predicted= 30\n",
            " Predicted=  9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3jotDShZNhG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9b2026-2750-47a8-9509-2dd4815b5372"
      },
      "source": [
        "import re\n",
        "\n",
        "#model_name = 'OutputData/SUB_ADD1637408951.4818375.h5'\n",
        "print(\"===== Loading Model ====\")\n",
        "\n",
        "model = tensorflow.keras.models.load_model(model_name)\n",
        "print(\"===== Model Loaded ====\")\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Loading Model ====\n",
            "===== Model Loaded ====\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFf67sYjL4zC"
      },
      "source": [
        "#User tests\n",
        "non_correct=True\n",
        "#expression = \"30-15-7+40-11-9-4+33-12+6\" \n",
        "expression=input('Enter your equestion , you can use only number or + and - : ')\n",
        "\n",
        "\n",
        "while non_correct :\n",
        "\n",
        "  regexp = re.compile(r'[^+\\-/^0-9\\s]')\n",
        "\n",
        "  if len(regexp.findall(expression)):\n",
        "      expression=input('re-Enter your equestion , you can use only number or + and - : ')\n",
        "  else:\n",
        "      non_correct=False\n",
        "\n",
        "print(\"Equation : \", expression)\n",
        "items=expression.replace(\" \",\"\")\n",
        "number_or_symbol = re.compile('(\\d+|[^ 0-9])')\n",
        "item=re.findall(number_or_symbol, items)\n",
        "\n",
        "i=0\n",
        "while(i<len(item)-2):\n",
        "\n",
        "  if(\"+\" in str(item[1])):\n",
        "    X=[[int(item[0]),int(item[2])]]\n",
        "    X=to_string_test(X,[], n_numbers, largest)\n",
        "\n",
        "  else :\n",
        "    X=[[int(item[0]),int(item[2])]]\n",
        "    X=to_string_test([],X, n_numbers, largest)\n",
        "   \n",
        "  X, _ = integer_encode(X,[] , alphabet)\n",
        "  X, _ = one_hot_encode(X, [], len(alphabet))\n",
        "  X = array(X)\n",
        "  result = model.predict(X, batch_size=n_batch, verbose=0)\n",
        "  predicted = [invert(x, alphabet) for x in result]\n",
        "  \n",
        "  new_equation=item[i+3:]\n",
        "  item=predicted+new_equation\n",
        "  i=0\n",
        "\n",
        "print(\"results =  \", item[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
